## 《小白学数据挖掘与机器学习（SPSS Modeler案例篇）》读书笔记

[TOC]

## 第一章  数据挖掘那些事儿

### 一、数据挖掘

* 数据科学是一门从数据中提炼知识及洞察趋势的科学，借助观察事物（获取数据），通过合适的手段（建立统计挖掘模型）来量化这些关系
* 可以将数据挖掘任务简单分为**预测任务**与**控制任务**
* 从预测场景的角度，可以把统计挖掘分为**有监督学习**和**无监督学习**
> 对于**有监督学习**
>> 目标变量属于定量变量（连续型变量），可以定义为回归问题
>> 目标变量属于定性变量（分类型变量），可以定义为分类问题

> 对于**无监督学习**
>
> > 只有自变量X，没有明确的因变量Y

### 二、CPISP-DM数据挖掘方法论
* CRISP-DM（Cross Industry Standard Process for Data Mining）跨行业数据挖掘标准流程
* 将数据挖掘项目划分为6个步骤：

**1. 商业理解**
> 在商业理解阶段，最重要的目标是明确实际业务需求，界定好需要解决的商业问题，同时要充分评估数据应用项目的实施，还需要考虑如何把商业问题转化为数学问题

**2. 数据理解**
> 在数据理解阶段，需要全面认识企业的数据资源，以及这些资源有何特征，并基于应用目标开展数据探索工作
>> 1. 哪些数据可以用来分拆本次的主题？
>> 2. 哪些数据已经在公司的系统中？
>> 3. 是否有一些重要的影响因素还没记录或者需要付出一定的代价才能获得？

> 在确定好分析的数据源后，还需要确定这些数据中每个指标的业务含义是什么，了解业务含义和统计口径对于后续分析非常重要，这决定了我们对数据的处理方式

> 在充分理解数据的业务含义后，还需要对数据进行探索性（描述性）分析
>> **数据质量分析**：包括对**缺失值、极值、离群值**的识别，也包括对分类字段中类别过于集中或类别数量特别多的字段进行分析
>> **数据分布**：可以借助**分布图、箱线图**查看数据的分布情况，查看数据分布是否符合业务的认知
>> **辅助统计指标**：可以计算指标的**算数平均数、中位数、四分位数**等常用的统计指标，也可以结合数据的**偏度和峰度**进行辅助分析
>> **统计分析**：可以计算**相关系数矩阵**（统计指标之间的关系），也可以结合 **t检验** 以及**卡方检验**进行一些变量筛选工作

**3. 数据准备**
> 在数据准备阶段，需要**整合不同的数据源**，然后**筛选、清洗、重构数据**，生成能够满足数据挖掘需要的材料

> **1） 数据清洗**
> > **缺失值**：**分类字段**处理可以选择众数，**连续字段**可以选择平均值/中位数或者通过回归进行插补
> > **离群值**：可以删除记录或把离群值进行替换，一般可以用如下公式代替
> > > 上离群值=Quantile[^1] (0.75) + IQR[^2]*3
> > >
> > > 下离群值=Quantile (0.25) - IQR*3 
> > > [^1]Quantile为四分位数
> > > [^2]IQR（Inter Quantile Range，四分位差）为上四分位与下四分位之差

>> **处理无效值**
>> **修改不合规字段**
>> **编码方式/统计口径不一致的问题**

> **2） 数据衍生转换**
>> **（1）单变量转单变量**
>> **连续型变量转换为连续变量**：一般是出于业务需要或计量比较需要进行转换；出于对数据分布修订的转换；为了更好地对比不同数量级的数据
>> **连续变量转换为离散变量**：一般是为了更好地应用于业务或者算法需要进行的转换，但是这种处理方式会损失一定的消息，一般采取的措施是利用分箱处理，可以选择等距离分箱或等数量分箱
>> **离散变量转换为连续变量**：一般只用于将一些有序的分类变量转换为1、2、3、4……
>> **离散变量转换为离散变量**：一般当某个分析变量中包含多个类别时，考虑到对模型会产生不良影响，会合并变量

>> **（2）多变量之间相互衍生** （简单计算所得）
>> **汇总型指标**
>> **强度相对指标**
>> **比例相对指标**：反映总体与各部分的比例关系
>> **时间对比指标**：同比、环比、定基比
>> **趋势型指标**：用来判断指标的趋势变化
>> **波动指标**：一般用标准差或变异系数来描述

**4. 建立模型**
> 建立模型阶段属于整个数据挖掘项目的核心阶段，需要根据**商业目标、数据特性及实际约束**，选择合适的建模技术建立模型，还需要明确建立模型的目标，其在很大程度上决定了要选择什么类型的模型

> 应该**结合数据特征、算法优势**，有针对性地选择合适的算法，一个数据挖掘项目往往需要通过**多次尝试**，才能找到合适的算法

**5. 模型评估**
> 在模型评估阶段，需要**从技术层面判断模型的效果**，以及**从业务层面判断模型的实用性**
>> **模型准确率**：直接描述模型的总体准确情况
>> **模型精确率**：主要反映了模型对目标类别的预测准确性
>> **模型召回率**：衡量模型能否将目标覆盖
>> **F值**：一般情况下都希望无论是模型精确率还是模型召回率都尽可能地高，但实际上，在模型优化上，这两个指标往往是相互制约的，为了综合考虑两个指标，可以**使用F值作为综合评价指标，$F_1$实际上是精确率和召回率的调和平均数**

**6. 结果部署**
> 需要把数据挖掘结果应用到实际的商业项目中以实现价值，同时也要**制定相应的维护及更新策略**

> **数据挖掘团队**与**业务团队**的紧密结合，能够**基于企业的实际情况选择合适的应用策略**


## 第二章  数据挖掘之利器：SPSS Modeler

### 一、主界面

* **模型流构建区**
> 可以完成**数据探索、数据清洗以及数据建模**等工作
> 可以在节点区**将节点拖拽**到模型流区中
> 一次数据挖掘的过程，就是由分析人员通过拖拽一个个节点完成的一系列过程

* **节点区**
> 模型流是由一系列节点连接而成的，这些**节点**都来源于节点区
1. **源节点**：包含了接入各种类型数据源的方式
2. **记录选项节点**：从行的角度处理数据
3. **字段选项节点**：从列的角度处理数据
4. **建模节点**：为用户提供数据挖掘模型的参数调整，可以供用户后续调用
    1） Analytics Server节点
    2） 分类节点
    3） 关联节点
    4） 细分节点（即聚类）
    5） 图形节点：通过图形展示的方法进行数据探索、结果展示和结果评估
    6） 输出节点：提供了多种数据及结果的展示形式
    7） 导出节点：把数据结果导出为各种格式的文件
    8） Statistics节点

* **模型流、结果和模型管理区**
1. **流**：通常会同时构建、编辑多个模型流，可以实现多个模型流之间进行切换
2. **输出**：在一次建模过程中，可能会产生多种结果
3. **模型**： 用户建立的所有模型

* **数据挖掘项目管理区**


### 二、鼠标操作

* **左键**：用于选择节点，之后按住不放可以拖动移动节点
* **右键**：打开快捷菜单
* **滚轮**：按住滚轮并移动鼠标可以用于节点之间的连接


## 第三章 巧妇难为无米之炊：数据，数据！

### 一、数据的身份

* 在数据挖掘的过程中，每一条记录对应一个对象，一条记录由一个或多个变量组成，一个变量可以从两个维度来考察：**测量级别和角色**

**1. 变量的测量级别**
* 变量按照测量级别可以分为**数值型变量**和**分类型变量**

 1） 默认型：变量的存储类型和取值范围均未知时
 2） 连续型：整数、实数或日期/时间
 3） 标记型：描述具有两个不同值的变量
 4） 名义型：描述具有多个不同值的变量
 5） 定序型：描述具有多个不同值的变量，这些值具有固定的顺序
 6） 分类型：已知变量为字符串型变量，但取值范围未知时
 7） 集合型：标志列表中记录的非地理空间数据
 8） 地理空间型：与“列表”存储类型配合使用以标志地理空间数据
 9） 无类型：一个变量含有超过250个以上不同取值时

**2. 变量的角色**

 1） 输入：自变量
 2） 目标：因变量
 3） 任意：同时作为自变量和因变量
 4） 无：不用于建模
 5） 分区：训练、测试和验证
 6） 拆分：仅用于分类（标志、名义、有序）变量
 7） 频率：仅用于数值变量
 8） 记录标志：模型忽略该变量

### 二、数据的读取

**1. 读取excel(.xls/.xlsx)文件数据**
* 源节点——>excel节点

**2. 读取变量(.csv/.txt)文件数据**
* 源节点——>变量文件

**3. 读取SPSS Statistics(.sav)文件数据**
* 源文件——>Statistics文件

**4. 读取数据库数据**
* 源文件——>数据库
* 要使用ODBC访问数据库，在使用数据库节点对数据进行读取或写入前，需要先**配置相关的ODBC数据源**
> 计算机——>控制面板——>管理工具——>系统和安全——>数据源（ODBC）——>系统DSN——>添加

### 三、数据的基本设定

**1. 变量角色的设定**
* 在选择数据文件后，SPSS Modeler会自动识别数据的存储类型，数据进入**存储**选项卡后，就自动进入**半实例化**
* 当字段处于半实例化的状态时，还没获取到各个字段的取值范围，单机**读取值**按钮完成对数据的**实例化**
* **清除值**：清除数据的值，即取值范围，但保留对测量级别的修改
* **清除所有值**：清除测量级别和值

**2. 字段的筛选及命名**
* 类型——>过滤——>字段名

### 四、数据的集成

* 数据的集成分为两种：**数据的变量集成（数据合并）**和**数据的记录集成（数据追加）**

**1. 数据的变量集成：合并节点**
* 数据的变量集成是**数据的横向合并**，是**针对原始数据表格横向增加列的过程**
* **当选择多个关键字时，当且仅当多个关键字取值一样时才合并数据**
* 链接类型
 1） 内部连接（inner join）
 2） 完全外部连接（full outer join）
 3） 部分外部连接（left outer join/right outer join）
 4） 反连接（except）

 **2. 数据的记录集成：追加节点**
* 数据的记录集成是**数据的纵向合并**，是**针对原始数据表格纵向增加行的过程**


## 第四章 一点都不简单的描述性统计分析

* 在数据理解阶段，需要从整体上进一步**认识数据的特征及数据的分布情况**，发现及把握数据的内在规律，从而为后续的数据准备及数据建模打下良好的基础

### 一、 分类变量的基本分析：“矩阵”节点

* **“矩阵”节点**是进行基本统计分析的最基本节点，可以**提示两个分类型变量之间是否存在明显的关联关系**

### 二、连续变量的基本分析：“数据审核”节点

**1. 连续变量基本分析指标**
  **1） 数据的集中趋势指标**
​          * 数据的集中趋势反映了数据向中心聚拢的程度，通过了解数据的中心位置能够让我们很好地了解数据的水平
​     **（1） 算数平均数**
​       * 算术平均虽好，但不能滥用
​       * 算术平均数**非常容易受到极端值的影响**，一般**去掉离群值再计算**或者**采用截尾均数**
​     **（2） 中位数**
​       * 要计算中位数，需要把数据按照从小到大的顺序排列，处在中间位置的数据即为所求的 中位数
​       * 中位数只利用了数据的顺序信息，对于整体数据的信息利用并不充分，因此常不单独用中位数，会搭配算术平均数及其它指标进行综合判断
​     **（3） 分位数**
​       * 最常用的分位数是百分位数和四分位数
​       * 分位数既可以描述数据的集中趋势，也可以描述数据的离散趋势
​       * 可以用**四分卫差**来描述数据的离散程度
​     **（4） 众数**
​       * 一组数据中出现次数最多的数据
​       * 当数据量较少的时候，众数的实用意义不大

  **2） 数据的离散趋势指标**
​    **（1） 极差**
​     * 反映数据的变动范围
​    **（2） 离差与平均差**
​     * 离差衡量单个数据的波动范围，平均差衡量总体数据的波动程度
​    **（3） 方差与标准差**
​     * 衡量总体数据的波动程度

  **3） 偏度和峰度**
  **（1） 偏度**
​    * 研究数据分布对称程度的统计量，能够判定数据分布的不对称程度及方向
​    * **数据分布对称时，偏度为0；偏度>0，数据分布右偏（正偏态）；偏度<0，数据分布左偏（负偏态）；偏度的绝对值越大，分布的便宜程度越严重**
  **（2） 峰度**
​    * 研究数据分布陡峭或平滑的统计量，能够判定数据分布相对于正态分布是更陡峭还是更平滑
​    * **正态分布的峰度为0，均匀分布的峰度为-1.2，指数分布的峰度为6
​    * 峰度>0，更陡峭；峰度<0，更平滑**
  **（3） 利用偏度和峰度进行正态性检验**
​    * **偏度系数与其标准误差的比值的绝对值>2，可以拒绝服从正态分布的假设
​    * 峰度系数与其标准误差的比值的绝对值>2，可以拒绝服从正态分布的假设**

**2. “数据审核”节点**
* 能够综合输出所有数据变量的汇总统计量、直方图和分布图的报告，可以快速、有效地初步理解数据
* 可以**一次性生成数据审核报告**
* **交叠字段是分类型变量**，生成的图形会显示基于该字段的不同分布；**交叠字段是连续型变量**，会额外生成两个变量的相关系数及对应的 t检验 结果


## 第五章 何为足够大的差异：常用的统计检验

* 按性质划分，变量之间的关系分为：**两个连续变量之间的关系、两个分类变量之间的关系、连续变量与分类变量之间的关系**

### 一、假设检验

**1. 假设检验的基本原理**
* 总体：所研究问题的全体对象
* 按照一定方法从总体中抽取部分研究对象（样本）进行研究
* 统计推断：由于总体分布未知，通过抽取样本数据进行测量，从而对总体进行推论的方法
* 假设检验是统计推断的重要组成内容，通过构造假设条件，通过样本数据对假设条件进行检验，从而得出结论的方法
* 假设检验的核心是判断这个差异是否满足以通过抽样的随机性来解释
* 第一个假设被成为**原假设（$H_0$）**,第二个假设被成为**备择假设（$H_1$）**
* 假设检验是一种**“小概率反证”**的思想，小概率事件的阈值α为检验水平，一般取**α=0.05**，即把发生概率小于0.05的事件称之为小概率事件
* **如果在假设检验中没有拒绝原假设，并不意味着完全接受原假设，只是说明样本数据的“证据”不足，暂时不拒绝原假设**

**2. 假设检验的一般步骤**
  **1） 建立假设检验**
​      * 原假设（$H_0$）、备择假设（$H_1$）、设定显著性水平α=0.05
  **2） 选择假设检验方法和计算检验统计量**
  **3） 判断临界值或者P值，得出结论**

* **当P值<α值，检验统计量>临界值，可以拒绝原假设**

### 二、连续变量与分类变量之间的关系：t检验

* **两组独立样本**是指样本分组采取的是完全随机设计方式，并没有针对分组有专门的配对
* **两组配对样本**是指两个组别之间是一一配对的

**1. 两组独立样本均值比较**
​    * 使用**独立样本 t检验**
**2. 两组配对样本均值比较**
  1）对同一个个体，分别接受不同处理或接受处理前后的比较 
  2） 按照一定的因子，将受试个体一一配对
**3. 使用 t检验 的前提条件**
  **1） 正态性**
​    * 各组样本应分别服从正态性假设
​    * **样本数大于50个**时，样本君子能够近似服从正态分布，不需要再进行特别检验
​    * 结果不符合正态性假设的前提，需要使用**非参数检验**的方法
  **2） 方差齐性**
​    * 借助**Leven's检验**进行验证
  **3） 独立性**
​    * 检验样本之间的观察应该相互独立
**4. SPSS Modeler实战**
* 两组独立样本均值比较，选择**“在字段的组之间”**单选框
* 两组配对样本均值比较，选择**“在字段对之间”**单选框
* 多组样本均值比较，选择**“在字段的组之间”**单选框，SPSS Modeler会**执行单因素方差分析**
* **重要性输出为1-P值**

### 三、两个连续变量之间的关系：相关分析

* 通过构造变量之间的**散点图**，以直观的形式观察**变量之间的关系**，缺点是**缺乏较为精准的量化**
* 为了能够科学地衡量**变量之间的相关关系的强弱**，可以使用**相关分析**

**1. 相关分析理论**
* 相关系数 r 是否具备足够的说明能力，也是需要检验的
* r 的取值介于-1~1，1代表完全正线性关系，-1代表完全负线性关系
* 0<|r|<1，即x与y之间存在一定的线性关系
* r >0，x与y为正相关； r <0，x与y为负相关
* r 是对两个变量之间线性相关程度的度量，r =0时，职能说明两个变量之间不存在线性关系，但不意味着两个变量之间不存在其它类型的关系（如非线性关系）
* **相关关系并不等于因果关系**
* **相关系数显著性检验**回答的是变量之间是否存在关系的问题，**相关系数**回答的是线性关系强弱问题
* 存在相关系数显著性通过了，但相关系数并不大的情况
**2. SPSS Modeler实战**
* 输出——>Statistics节点
* 主要用于分析连续变量，能快速计算多个连续变量的一系列统计指标，也可以计算这些连续变量之间的相关关系
* **按重要性（1-P）定义相关强度**：得到的值越接近1，代表两个变量的相关性越大（即非独立）
* **按绝对值定义相关强度**：基于Pearson（皮尔逊）相关性的绝对值，得到的值越接近于1，变量之间的相关性越强
* **第一种方法评价变量之间是否有关系，第二种方法评价变量之间的关系强弱**

### 四、两个分类变量之间的关系：卡方（$χ^2$）检验

* 两个分类变量之间的独立性检验问题可以使用卡方检验

**1. 卡方检验的原理**
* 一般可以使用**列联表**分析两个分类变量之间的关系，可以列出交互属性的频数，卡方检验是基于列联表对变量的独立性进行检验的
**2. 卡方检验的前提条件**
* 只有当列联表中的**样本总量大于或等于40，并且每个单元格的理论频数均大于5时，才可以直接计算**
* 如果**样本总量大于或等于40，但存在单元格的理论频数$E_i$小于5并且大于1时，需要使用连续型校正公式**
* 如果列联表数据还不符合要求，即**样本总量小于40，或至少存在一个单元格的理论频数$E_i$小于1时，需要用Fisher确切概率法进行计算**
**3. SPSS Modeler实战**
* “矩阵”节点——>概率（P值）


## 第六章 从身高和体重的关系谈起：回归分析

* 在**有监督学习中**，根据响应变量的种类，可以分为**回归（因变量 y 为连续变量）**与**分类（因变量 y 为分类变量）**

### 一、一元线性回归

* 回归分析是利用一个或多个自变量x，通过**拟合**适当的回归方程来预测因变量y的方法
* 借助回归分析，能够很好地**量化描述自变量x与因变量y的关系**，同时也可以**预测因变量y**
* 一元线性回归分析是在回归方程中含有一个自变量x

> 一元线性回归分析的步骤
>> （1） 分析因变量与自变量关系，构建回归模型
>> （2） 对模型参数进行估计，求解回归模型
>> （3） 对模型参数进行检验，确认模型有效性
>> （4） 拟合优度检验，判断模型解释能力
>> （5） 借助回归模型进行预测

**1. 分析因变量与自变量的关系，构建回归模型**
**2. 估计模型参数，求解回归模型**
**3. 对模型系数进行检验，确认模型有效性**
**4. 拟合优度检验，判断模型解释能力**
* 要判断回归方程对样本数据的拟合程度，可以用**决定系数**进行相应的度量
* **总平方和**：样本观测值与其均值只差的平方，**反映因变量y的波动情况**，记为**SST**
* **回归平方和**：通过**自变量x所引起因变量y的波动情况**，记为**SSR**
* **残差平方和**：**不能被构建的回归方程所解释的波动**，记为**SSE**
* **SST=SSR+SSE**
* **SSR**：能够通过自变量x解释的部分
* **SSE**：不能由自变量x解释的部分
* SSR所占的比重越大，则SSE越小，**证明回归的效果越好**
* **决定系数**：SSR与SST的比值，记为**$R^2$**
**5. 借助回归模型进行预测** 

### 二、多元线性回归分析

> 多元线性回归分析的步骤
>> （1） 分析因变量与自变量关系，构建回归模型
>> （2） 对模型参数进行估计，求解回归模型
>> （3） 对模型参数进行检验，确认模型有效性
>> （4） 拟合优度检验，判断模型解释能力
>> （5） 模型的变量选择
>> （6） 借助回归模型进行预测

**1. 估计模型系数，求解回归模型**
* 当自变量之间存在精确相关关系或高度相关关系时（即**多重共线性**），或者**样本数量小于变量数量**时，需要使用**主成分分析**或**引入正则化**

**2. 对模型参数进行检验，确认模型有效性**

* 通过原假设，F检验只能说明整体自变量x与因变量y之间有关系，但并不能说明哪个自变量x是否与因变量y有关系，仍**需要 t 检验来判断每个自变量的显著性**

**3. 拟合优度检验，判断模型解释能力**
* $R^2$存在明显的不足：**自变量越多，不管新增的自变量本身是否真的有效，$R^2$总是不减的**
* 为避免这种情况，需要在决定系数公式中**引入惩罚项**，称为**调整决定系数**

**4. 模型的变量选择**
 **1） 前进法**

 * 前进法是自变量由少到多的过程，根据自变量准入标准，每一步引入一个当前最重要的自变量，直至引入所有合乎标准的自变量

 **2） 后退法**
 * 后退法是自变量由多到少的过程，先利用全部自变量建立全模型回归方程，再利用检验标准逐个剔除最不重要的自变量

 **3） 逐步回归法**
 * 在前进法的基础上进行改进，每当回归方程中引入新的自变量后，都对方程中现有的自变量重新检验，当发现有自变量不显著时，将其重新剔除

### 三、使用线性回归分析的注意事项

**1. 线性**
* 自变量x与因变量y是线性关系，可以通过**散点图**判断自变量与因变量之间的线性关系

**2. 独立性**
* $y_i$之间的取值相互独立，即每一个观测的取值不受其它观测的影响，可以通过制作**残差图**判断其是否满足独立性要求

**3. 正态性**
* 在给定$x_i$的情况下，随机变量$y_i$也服从正态分布，可以通过**正态概率图**判断其是否服从正态性

**4. 方差齐性**
* 对应不同的$x_i$，随机变量$y_i$的方差均相同，可以通过**残差图**判断$y_i$是否为等方差

### 四、SPSS Modeler实战

* **进入法**：把所有输入变量均作为自变量建立模型
* **步进法**：即用逐步回归法建立模型
* **前进法**
* **后退法**


## 第七章 回归岂止这么简单：回归模型的进一步扩展

### 一、曲线回归

### 二、Logistics回归

* 当因变量与自变量的关系表达式不再是线性时，通过引入衍生变量，使其转换为线性表达式
* **在没有任何先验条件下，阈值一般设为0.5**，当有进一步明确需求的时候，阈值也是可以调整的
* **Logistics函数**的图形是一个典型的**“S”形曲线**，并且它的**值域在[0,1]区间**


## 第八章 模型评估那些事儿：过拟合与欠拟合

* 在构建好模型后，还需要充分评估模型
* 为了能够准确评估模型，需要将数据集划分为**训练集、测试机（和验证集）**

### 一、过拟合与欠拟合

* 一般将**60%-80%**的数据用于训练，将剩下的数据用于测试
* **训练误差**：训练数据集中产生的误差
* **测试误差**：测试数据集中产生的误差
* **通过比较测试误差的大小来选择模型**
* 在通常情况下，训练数据集的准确率都是高于测试数据集的，但训练数据集的预测准确率并不能很好地评估模型的预测能力
* **欠拟合**：对变量考虑不足或者对模型形式估计不足，**连训练数据的基本特征都不能够很好拟合的情况**
* **相对适合**：**模型在训练数据集及测试数据集中的误差可能都比较小**
* **过拟合**：**把训练数据集中的数据的所有特性（包括噪声特性）都学习到的状态**
* **模型越复杂，出现“过拟合”的可能性就越大**
* 奥卡姆剃刀原理：如无必要，勿增实体
* 若有两个预测能力相当的模型时，应当选择其中较为简单的一个
* 产生欠拟合的原因：**所选择的数据特征不足**或者**所选择的学习算法学习能力不够强大**
* 为了准确评估模型的性能，把数据集分成三个部分，**用于模型训练，得到估计参数（训练数据集），用于模型评估，得到评估结果（测试数据集），用于选择具体的超参数（验证数据集）**
* 划分数据集方法：**留出法、分层抽样法**
* **留出法**只是直接切割划分，**可能会为模型带来一定的不确定性，可以选择交叉验证（CV）代替**

### 二、留出法与交叉验证

**1. 留出法与分层抽样**
* **留出法**直接将数据集划分为两个数据集：训练数据集和测试数据集，采用完全随机的抽样方法，**可能会由于划分的问题而改变原有的数据分布**，为避免发生这种情况，**可以采用分层抽样的方法**
* **重复抽样**：考虑到只进行一次随机抽样划分训练数据集和测试数据集可能会存在较大的不稳定性，将抽样结果重复 n 次，把 n 次结果加总后求平均值

**2. 交叉验证**
* **交叉验证是参数调整过程中非常重要的一个方法**，多使用** k 折交叉验证**
* 一般常用**10折交叉验证**，设定 k =10
* 可以采用不同的划分方式重复进行交叉验证
* **留一交叉验证**：交叉验证的一种特殊情况，**优点**是每次训练数据集都与原始数据非常接近，并且也能做到训练数据集和测试数据集是对立的，可以保证得到的结果相对比较准确，**缺点**是计算量会大大增加


## 第九章 从看电影的思考到决策树的生成

### 一、决策树概述

* **决策树**算法**具有良好的分类能力和较强的可解释能力**，有直观、易懂的特点
* 决策树算法实际上是一种根据训练数据集，通过一系列测试问题，输出分类结果进行判断的过程
* **根节点**和**内部节点**代表相应的测试条件，**叶节点代表**最终输出结果
* 树状表达形式与 If - Then 规则可以相互转换，从根节点出发，到任意一个叶节点将形成一条规则
* 通过决策树所形成的**规则应当是互斥且完备（相互独立，可以穷尽）的**，即对于任意一个样本数据，有且只有一条规则与其对应输出分类结果

### 二、决策树生成

* 费雪提供了一个机器学习领域中最著名的分类数据集——**鸢尾花**数据集
* 在决策树生成的过程中，对于数据集的每一次划分，应该选择可以令子集纯度更高的划分条件，随着数据集不断被划分，子集的纯度越来越高，直至该节点下的样本都属于同一个类别
* 在决策树生长中，重点在于“纯度”的计算公式，也就是怎么选择最优特征过程

**1. 从ID3算法到C5.0算法**
* 使用**信息熵**来衡量集合的纯度
* **信息熵**用来描述信息的混乱程度或者信息的不确定性
* 需要比较父节点与子节点之间的纯度差异，这种差异越大，说明测试条件越好，**信息增益**则是这种差异的判断标准
* 利用信息增益公式，划分的子集数量越多，熵会倾向于更小，**划分类别较多的输入变量更加容易被选为划分条件**
* 当对划分条件的信息增益加上了**“代价”**的考虑时，类别较多的划分条件的信息增益率减少了

**2. CART算法**
* CART算法即**分类与回归树**
* **CART算法**不仅能够处理分类型变量，也能处理连续型变量
* **ID3系列算法**只能处理分类型变量，即只能建立分类树
* **CART算法**只能建立二叉树，**ID3系列算法**能够建立多叉树
* **CART算法**会采用对多类别进行合并的原则，从而输出二叉树
* **CART算法**采用基尼指数（分类树）及方差（回归树）度量纯度，**ID3系列算法**采用信息熵度量纯度

 **1） 分类树**
 * 与信息熵类似，**基尼指数**越低，代表数据集的纯度越高

 **2） 回归树**
 * 对于数值型变量，方差就是一个很好衡量数据集差异度或者纯度的指标

### 三、决策树的剪枝

* 为了解决决策树的过拟合问题，需要引入决策树的剪枝策略：**前剪枝**和**后剪枝**

**1. 预剪枝策略**
* 预剪枝策略是在决策树生长的过程中，通过多种方式来限制决策树的生长
* **决定决策树节点样本数量的下限**：指定父节点样本数量的下限；指定子分支节点样本数量的下限
* **限制决策树生长的深度**：**C5.0算法**的策略是限制子分支节点的最小记录数；**CART算法**的策略更为丰富，不但提供了父分支及子分支的最小记录数或样本比例，也提供了限制决策树生长深度的方法

**2. 后剪枝策略**
* 后剪枝策略允许决策树在充分生长的基础上，根据对预测误差的估计
* **基于误差估计的剪枝**：原理就是**减小误差**，可以使用测试样本的预测误差作为该节点的误差估计，**C5.0算法**直接采用训练数据集的悲观误差作为误差估计
* **基于误差代价——复杂度的剪枝**：**模型的高精度和模型的复杂度是相互制衡的**，为了获得一个能够满足基本预测精度，但是模型复杂度又不是很高的模型，需要在两者之间做出权衡，一个直观的判断标准是误差代价——复杂度，**CART算法**是使用一种基于误差代价——复杂度的后剪枝策略进行剪枝

**3. 代价敏感学习**
* **误判成本往往是不一致的**
* 在构建代价敏感矩阵的基础上，在建模的过程中损失函数将不再仅仅基于错误率的下降，而是需要结合误判成本进行考虑

### 四、SPSS Modeler实战

* **组符号**：将尝试对分组变量的相似类别进行合并
* **使用Boosting**：使用Boosting技术生成多棵将决策树，通过组合投票的方式提供模型的准确率
* **“成本”选项卡**主要用于设置分类算法的代价敏感矩阵

### 五、关于信息熵的扩展

* **信息熵的表达式应该满足以下条件：**
> 1） 信息量和事件发生的概率有关，事件发生的概率越低，传递的信息量越大
> 2） 信息量应当是非负的，必然发生的事件的信息量为零
> 3） 两个事件的信息量可以相加，并且两个独立事件的联合信息量应该是它们各自信息量的和

* **信息熵就是信息量的数学期望**，信息熵的**特点**如下：
> 1） 信息熵与事件的可能性有关，在概率均等的情况下，存在的可能性越多，信息熵越大，信息也就越不确定
> 2） 信息熵与事件的概率分布情况有关，概率分布越平均，信息熵越大，当所有概率分布均等时，信息熵达到最大值


## 第十章 人工神经网络：从人脑神经元开始

### 一、从人脑神经元带人工神经网络

* 每个神经元的信息处理过程可以分为三个部分：**输入、处理、输出**
* 人工神经网络的三个层级：**输入层、隐藏层、输出层**，隐藏层可以是零层或多层
* 输入层的神经元作为第一层，只负责输入信息，隐藏层和输出层的神经元接收上层节点的输出作为自身的输入，档输入信息超过一定的阈值时，将激活从而向其它神经元输出信号

### 二、感知机

* **感知机可以区分线性可分数据**，但由于网络中只含有两层神经元，**当面度线性不可分和非线性可分问题时，感知机无能为力**，并不能找到一个线性超平面将两类数据进行分类

### 三、人工神经网络

* 由于感知机只有一层用于计算的神经元，对于线性不可分问题无能为力，未解决此问题，可以在输入层及输出层的中间加入隐藏层，形成多层神经网络结构

**1. 隐藏层的作用**

**2. 人工神经网络算法**

* **反向传播算法**，简称BP算法，是建立在梯度下降算法基础上适用多层神经网络的参数训练方法

### 四、SPSS Modeler实战

* **“基本”选项组**：在SPSS Modeler中提供了两种人工神经网络模型算法：**多层感知机（MLP）、径向基函数（RBF）**
* 相比多层感知机模型，径向基函数至多只能包含一个隐藏层，并且预测能力相对弱一点
* **隐藏层——自动计算单元格数**：该方法只能构建单隐藏层的人工神经网络
* **隐藏层——定制单元格数**：在类神经网络中，最少包括一个隐藏层，最多包括两个隐藏层
* **“中止规则”选项组**：满足条件**使用最大训练时间、定制最大训练周期数量、使用最低准确性时**，算法可以停止训练；**当算法无法进一步降低误差时**，也将停止训练
* **“高级”选项卡**
> **1） 过度拟合防止集合**：支持从已经划分好的训练数据集中抽取独立的样本作为测试数据集，**用于错误率的检验**
> **2） 复制结果**：通过设置随机种子复制分析，从而能够重现分析结果
> **3） 预测变量中的缺失值**：**成列删除、插补缺失值**
>> **对于连续型变量**，将插补最小值和最大值的平均值
>> **对于分类型变量**，将插补最常出现的类别


## 第十一章 物以类聚，人以群分：聚类分析

### 一、聚类思维的概述

* 聚类算法是一种**无监督学习算法**

### 二、聚类算法的关键：距离

* 距离的衡量，一般选用**明氏距离**
* **当p=1时，明氏距离即曼哈顿距离**
* **当p=2时，明氏距离即欧式距离**
* **当p->∞时，明氏距离即切比雪夫距离**

### 三、K-Means算法

**1. K-Means算法原理**
* K-Means算法也称K均值算法，是一种属于原型的聚类算法，即在样本空间中找到具有代表性的质心，通过度量距离的方式，把每个样本分配到距离它最近的质心的类群中
* **聚类数K需要事先指定**，在实际操作中，需要根据实际的业务规则以及聚类效果综合比较
* **K-Means算法选择欧式距离**
* 通常**以质心不再发生改变作为停止条件**，一般情况下，质心可能需要迭代很多次才能停止发生改变，停止条件也常设为：**指定迭代次数、指定质心变动范围阈值ε**

**2. 轮廓系数**
* **轮廓系数作为聚类结果的衡量方法**
* 轮廓系数的取值范围为[-1,1]，数值越接近于1，说明模型分群效果越好
* **当轮廓系数>0.5时，分群效果较好；当轮廓系数<0.2时，分群效果不明显**

### 四、SPSS Modeler实战


## 第十二章 啤酒+尿布=关联分析？

### 一、一个关于关联分析的传说

### 二、关联分析的基本概念

* 关联格式可以有两种数据形式：**表格格式、事务格式**
* 在表格格式中，**每一个项目都单独作为一列属性变量，变量取值0和1分别代表没有购买和购买，每一行代表一个事物的完整集合**，由于每一个项目都单独成列，整个数据表格变得非常“宽”
* 事务格式只包含两个字段，**TID标志字段、项目内容字段，每条记录代表了单个项目**，类似于超市购物小票的记录方式

### 三、关联规则的有效性指标

* **支持度**反映了该规则的普遍程度
* **置信度**是在给定前项X的前提下，后项Y的条件概率
* 一般来说，**一个“好”的关联规则应当同时具有较高的支持度和置信度**
* 在实际使用过程中，一般都**会设置最小支持度和最小置信度**
* **置信度应当大于或等于支持度**
* **如果某条规则的支持度高，而置信度略高于支持度，说明该规则可信度较低，前项和后项的关系不明显**
* **如果某一个规则的置信度高而支持度低，说明该规则可靠性差，可能不具备推广应用的价值**
* **提升度**是规则的置信度和后项支持度的比值，反映了相比于总体，后项Y受到前项X的影响程度
* 一般**提升度越大，正向影响程度越高**；当提升度>1时，认为前项度后项是具有正向影响的；当提升度<1时，认为前项对后项是具有负向影响的
* 规则的**部署能力**就是规则前项的支持度减去规则的支持度，反映了已经购买条件（前项）但还没购买结果（后项）的客户比例

### 四、Apriori算法

* **算法步骤：**
> 1） 设定最小支持度$S_min$及最小置信度$C_min$
> 2） 根据最小支持度，生成频繁项集
> 3） 根据最小置信度，基于频繁项集，生成最终关联规则

**1. 生成频繁项集**
* 如果一个项集是频繁的，那么它的所有子集也一定是频繁的
* 如果一个项集是非频繁的，那么它的所有超集也是非频繁的

**2. 生成关联规则**

### 五、SPSS Modeler实战

**1. “字段”选项卡**
* 商品项的角色被设为**“输入”**，该项目在关联分析中只会作为前项出现
* 商品项的角色被设为**“目标”**，该项目在关联分析中只会作为后项出现
* 商品项的角色被设为**“任意”**，该项目在关联分析中既会作为前项出现也会作为后项出现
* 如果数据源是事务格式，则需要**选择“使用事务处理格式复选框”**，然后根据实际情况，设定“标志”和“内容”即可

**2. “模型”选项卡**
* **最低条件支持度**：SPSS Modeler使用的支持度下限阈值时前项支持度
* **仅包含标志变量的true值**：**仅适用于表格型数据**，选中此复选框，只显示项目为“真”的规则，我们更加关心关联购买的规则，**一般会选中此复选框**
* SPSS Modeler默认只显示支持度和置信度，为了能够更充分地评估结果，**在“显示/隐藏条件”菜单中，选中“全部选择”选项**
* **增益**表示**提升度**


## 第十三章 三个臭皮匠，赛过诸葛亮：集成学习算法

### 一、集成学习算法概述

* 当使用同一种算法生成时，称为**同质集成**；当使用不同种算法生成时，称为**异质集成**
* **要想集成有效，必须满足两个条件：**
> 1） 基分类器的准确率要大于一定的阈值，一般**大于0.5**
> 2） 基分类器之间尽可能独立

* 对于集成学习算法，**基于“少数服从多数”的投票原则**，以基分类器的多数项作为最终的预测值
* 由于分类器都是基于同样的数据训练出来的，**事实上根本不可能满足完全独立**，尽管并不能保证基分类器之间相互独立，但是可以通过构造方法，尽可能地保证基分类器的差异性

### 二、3种不同的集成学习算法

**1. Bagging算法**
* Bagging算法又称为**袋装法**，是一种借助从原始数据集中重复抽样生成新的训练数据集，再基于不同的训练数据集构建多个基分类器的方法
* 对于分类问题，可以采取“少数服从多数”的投票原则，**当出现平票时，可以结合模型的置信度对预测结果进行加权投票，或出现平票的结果中随机选择一个以输出最终结果**；**对于回归问题，可以对N个预测值求平均值**
* 对训练数据集$D_i$来说，它包含了原始数据集D中大约**63.2%**的样本

**2. Boosting算法**
* Boosting算法又称为**提升法**，将在每次迭代中赋予那些被基分类器“错判”的样本更高的权重，从而使得后续的基分类器能够给予这些样本更多的关注
* 通过不断调整样本的抽样权重，构造新的基分类器，如此反复，直至生成N个基分类器，最终基于N个基分类器进行加权集成输出结果
* Bagging算法与Boosting算法的一个**核心差别在于权重**

**3. 随机森林**
* 随机森林**是一门专为决策树而设计的集成学习算法**，是由很多棵决策树组合而成的“森林”
* 随机森林在Bagging算法对数据集进行抽样的基础上，**在生成决策树的过程中引入随机选择变量这一方式**
* 在SPSS Modeler中，随机森林算法的基分类器是**C&RT**
* 随机森林使用**自助法**进行抽样，因而对于每个基分类器来说大约有**36.8%**的样本没有被用来训练，这部分的样本被称为**“袋外数据”**
* 在SPSS Modeler中，随机森林算法的**预测准确性正是基于袋外数据的估计结果**

### 三、SPSS Modeler实战

* 在SPSS Modeler中，Bagging算法与Boosting算法属于**辅助学习技术**
* SPSS Modeler并没有把Bagging算法和Boosting算法单独封装为一个节点，而是作为某些节点（如C&RT节点）的选项
* 随机森林是作为单独节点**“Random Trees”**使用

**1. Bagging算法和Boosting算法**
* **预测变量频率**：**顶端的变量**是在所有组件模型中出现频率最多的预测变量，一般来说，出现频率最高的预测变量**通常都属于最重要的变量之一**

**2. 随机森林**
* **“基本”选项卡**
> 1） 样本大小：在默认情况下，**自助法的样本数量等于原始数据集的样本数量**
> 2） 处理不平衡数据：当**目标变量属于分类变量**时，可能存在数据不平衡的问题

**3. 集成学习算法结果比较**
